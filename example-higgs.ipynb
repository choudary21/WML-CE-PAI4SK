{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HIGGS example using IBM PowerAI Snap ML\n",
    "\n",
    "In this example we will train a Logistic Regression model on the HIGGS dataset, using both scikit-learn and snap-ml-local.\n",
    "\n",
    "The HIGGS dataset is avaliable in the UCI machine learning repository.\n",
    "\n",
    "Update device_ids list in LogisticRegression of snap_ml based on the number of GPUs available for you.\n",
    "\n",
    "To avoid 'kernel restart' problem increase CPU and memory for the jupyter environment (e.g. memory 10GB, CPU 100) and restart it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download input data\n",
    "\n",
    "Two wget commands are given below for downloading input HIGGS dataset - one for reduced dataset and another for bigger/full dataset. Many times better perfomance of snapML training is seen with bigger dataset.\n",
    "\n",
    "You can comment the line containing the wget command to avoid downloading dataset again if running the same wget command more than once. Similarly preprocessing code can be commented out second time onwards if using the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 3 times better training time with snapML compared to sklearn with this full HIGGS dataset\n",
    "#!mkdir -p data; cd data; wget -nc https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/HIGGS.bz2; bunzip2 HIGGS.bz2; cd ../\n",
    "\n",
    "\n",
    "#!mkdir -p data; cd data; wget -nc https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz; gunzip HIGGS.csv.gz; cd ../\n",
    "    \n",
    "\n",
    "# Download reduced dataset\n",
    "#!mkdir -p data; cd data; wget -O HIGGS -nc https://ibm.box.com/shared/static/v684mqemrrz9o9gsko4ox5l30t6ncqag; cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: b'1.000000000000000000e+00,8.692932128906250000e-01,-6.350818276405334473e-01,2.256902605295181274e-01,3.274700641632080078e-01,-6.899932026863098145e-01,7.542022466659545898e-01,-2.485731393098831177e-01,-1.092063903808593750e+00,0.000000000000000000e+00,1.374992132186889648e+00,-6.536741852760314941e-01,9.303491115570068359e-01,1.107436060905456543e+00,1.138904333114624023e+00,-1.578198313713073730e+00,-1.046985387802124023e+00,0.000000000000000000e+00,6.579295396804809570e-01,-1.045456994324922562e-02,-4.576716944575309753e-02,3.101961374282836914e+00,1.353760004043579102e+00,9.795631170272827148e-01,9.780761599540710449e-01,9.200048446655273438e-01,7.216574549674987793e-01,9.887509346008300781e-01,8.766783475875854492e-01'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-44df8afa814c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdefaultPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_svmlight_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaultPath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/data/HIGGS.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Make the train-test split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/powerai_env2/lib/python3.6/site-packages/sklearn/datasets/svmlight_format.py\u001b[0m in \u001b[0;36mload_svmlight_file\u001b[0;34m(f, n_features, dtype, multilabel, zero_based, query_id, offset, length)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \"\"\"\n\u001b[1;32m    146\u001b[0m     return tuple(load_svmlight_files([f], n_features, dtype, multilabel,\n\u001b[0;32m--> 147\u001b[0;31m                                      zero_based, query_id, offset, length))\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/powerai_env2/lib/python3.6/site-packages/sklearn/datasets/svmlight_format.py\u001b[0m in \u001b[0;36mload_svmlight_files\u001b[0;34m(files, n_features, dtype, multilabel, zero_based, query_id, offset, length)\u001b[0m\n\u001b[1;32m    288\u001b[0m     r = [_open_and_load(f, dtype, multilabel, bool(zero_based), bool(query_id),\n\u001b[1;32m    289\u001b[0m                         offset=offset, length=length)\n\u001b[0;32m--> 290\u001b[0;31m          for f in files]\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     if (zero_based is False or\n",
      "\u001b[0;32m~/anaconda3/envs/powerai_env2/lib/python3.6/site-packages/sklearn/datasets/svmlight_format.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    288\u001b[0m     r = [_open_and_load(f, dtype, multilabel, bool(zero_based), bool(query_id),\n\u001b[1;32m    289\u001b[0m                         offset=offset, length=length)\n\u001b[0;32m--> 290\u001b[0;31m          for f in files]\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     if (zero_based is False or\n",
      "\u001b[0;32m~/anaconda3/envs/powerai_env2/lib/python3.6/site-packages/sklearn/datasets/svmlight_format.py\u001b[0m in \u001b[0;36m_open_and_load\u001b[0;34m(f, dtype, multilabel, zero_based, query_id, offset, length)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mactual_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 _load_svmlight_file(f, dtype, multilabel, zero_based, query_id,\n\u001b[0;32m--> 178\u001b[0;31m                                     offset, length)\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# convert from array.array, give data the right dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/datasets/_svmlight_format.pyx\u001b[0m in \u001b[0;36msklearn.datasets._svmlight_format._load_svmlight_file\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: b'1.000000000000000000e+00,8.692932128906250000e-01,-6.350818276405334473e-01,2.256902605295181274e-01,3.274700641632080078e-01,-6.899932026863098145e-01,7.542022466659545898e-01,-2.485731393098831177e-01,-1.092063903808593750e+00,0.000000000000000000e+00,1.374992132186889648e+00,-6.536741852760314941e-01,9.303491115570068359e-01,1.107436060905456543e+00,1.138904333114624023e+00,-1.578198313713073730e+00,-1.046985387802124023e+00,0.000000000000000000e+00,6.579295396804809570e-01,-1.045456994324922562e-02,-4.576716944575309753e-02,3.101961374282836914e+00,1.353760004043579102e+00,9.795631170272827148e-01,9.780761599540710449e-01,9.200048446655273438e-01,7.216574549674987793e-01,9.887509346008300781e-01,8.766783475875854492e-01'"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "defaultPath = \".\"\n",
    "\n",
    "X,y = load_svmlight_file(defaultPath + \"/data/HIGGS.csv\")\n",
    "\n",
    "# Make the train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Convert to numpy ararys\n",
    "import numpy as np\n",
    "X_train = np.array(X_train.todense())\n",
    "X_test  = np.array(X_test.todense())\n",
    "\n",
    "# Normalize the training data\n",
    "from sklearn.preprocessing import normalize\n",
    "X_train = normalize(X_train, axis=1, norm='l1')\n",
    "X_test  = normalize(X_test,  axis=1, norm='l1')\n",
    "\n",
    "# Save the dense matrices\n",
    "np.save(defaultPath + \"/data/HIGGS.X_train\", X_train)\n",
    "np.save(defaultPath + \"/data/HIGGS.X_test\",  X_test)\n",
    "\n",
    "# Save the labels\n",
    "np.save(defaultPath + \"/data/HIGGS.y_train\", y_train)\n",
    "np.save(defaultPath + \"/data/HIGGS.y_test\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating a Logistic Regression Model using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluating a Logistic Regression Model using GPU\n",
    "from scipy import sparse\n",
    "\n",
    "# Load the data\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "defaultPath = \".\"\n",
    "\n",
    "t0 = time.time()\n",
    "X_train = np.load(defaultPath + \"/data/HIGGS.X_train.npy\")\n",
    "X_test  = np.load(defaultPath + \"/data/HIGGS.X_test.npy\")\n",
    "y_train = np.load(defaultPath + \"/data/HIGGS.y_train.npy\")\n",
    "y_test  = np.load(defaultPath + \"/data/HIGGS.y_test.npy\")\n",
    "print(\"Data load time (s):  {0:.2f}\".format(time.time()-t0))\n",
    "\n",
    "# Import the LogisticRegression from snap.ml\n",
    "from snap_ml import LogisticRegression\n",
    "lr = LogisticRegression(use_gpu=True, max_iter=15, dual=True, num_threads=32, device_ids=[0,1])\n",
    "\n",
    "# Training\n",
    "t0 = time.time()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"[snap.ml] Training time (s):  {0:.2f}\".format(time.time()-t0))\n",
    "\n",
    "# Inference\n",
    "proba_test = lr.predict_proba(X_test)\n",
    "\n",
    "# Evaluate log-loss on test set\n",
    "from sklearn.metrics import log_loss\n",
    "logloss_snap = log_loss(y_test, proba_test)\n",
    "print(\"[snap.ml] Logarithmic loss:   {0:.4f}\".format(logloss_snap))\n",
    "\n",
    "# Import the LogisticRegression from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(fit_intercept=False, dual=True)\n",
    "\n",
    "# Training time\n",
    "t0 = time.time()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"[sklearn] Training time (s):  {0:.2f}\".format(time.time()-t0))\n",
    "\n",
    "# Inference\n",
    "proba_test = lr.predict_proba(X_test)\n",
    "\n",
    "# Evaluate log-loss on test set\n",
    "logloss_sklearn = log_loss(y_test, proba_test)\n",
    "print(\"[sklearn] Logarithmic loss:   {0:.4f}\".format(logloss_sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating a Logistic Regression Model using CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluating a Logistic Regression Model using CPU\n",
    "from scipy import sparse\n",
    "\n",
    "# Load the data\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "defaultPath = \".\"\n",
    "\n",
    "t0 = time.time()\n",
    "X_train = np.load(defaultPath + \"/data/HIGGS.X_train.npy\")\n",
    "X_test  = np.load(defaultPath + \"/data/HIGGS.X_test.npy\")\n",
    "y_train = np.load(defaultPath + \"/data/HIGGS.y_train.npy\")\n",
    "y_test  = np.load(defaultPath + \"/data/HIGGS.y_test.npy\")\n",
    "print(\"Data load time (s):  {0:.2f}\".format(time.time()-t0))\n",
    "\n",
    "# Import the LogisticRegression from snap.ml\n",
    "from pai4sk import LogisticRegression\n",
    "lr = LogisticRegression(use_gpu=False, max_iter=15, dual=True, num_threads=32, device_ids=[])\n",
    "\n",
    "# Training\n",
    "t0 = time.time()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"[snap.ml] Training time (s):  {0:.2f}\".format(time.time()-t0))\n",
    "\n",
    "# Inference\n",
    "proba_test = lr.predict_proba(X_test)\n",
    "\n",
    "# Evaluate log-loss on test set\n",
    "from sklearn.metrics import log_loss\n",
    "logloss_snap = log_loss(y_test, proba_test)\n",
    "print(\"[snap.ml] Logarithmic loss:   {0:.4f}\".format(logloss_snap))\n",
    "\n",
    "# Import the LogisticRegression from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(fit_intercept=False, dual=True)\n",
    "\n",
    "# Training time\n",
    "t0 = time.time()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"[sklearn] Training time (s):  {0:.2f}\".format(time.time()-t0))\n",
    "\n",
    "# Inference\n",
    "proba_test = lr.predict_proba(X_test)\n",
    "\n",
    "# Evaluate log-loss on test set\n",
    "logloss_sklearn = log_loss(y_test, proba_test)\n",
    "print(\"[sklearn] Logarithmic loss:   {0:.4f}\".format(logloss_sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy; Copyright IBM Corporation 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
